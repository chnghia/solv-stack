# SOLV Stack: The Enterprise Local AI Infrastructure

**SOLV Stack** (**S**earXNG, **O**penWebUI, **L**iteLLM, **V**LLM) lÃ  má»™t giáº£i phÃ¡p háº¡ táº§ng AI "Local-First" trá»n gÃ³i, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ triá»ƒn khai LLM & Agentic Workflow trong mÃ´i trÆ°á»ng doanh nghiá»‡p vá»›i yÃªu cáº§u cao vá» báº£o máº­t, hiá»‡u nÄƒng vÃ  kháº£ nÄƒng má»Ÿ rá»™ng.

TÆ°Æ¡ng tá»± nhÆ° tinh tháº§n cá»§a **XAMPP** dÃ nh cho Web Dev, **SOLV Stack** Ä‘Ã³ng gÃ³i cÃ¡c cÃ´ng nghá»‡ AI SOTA (State-of-the-art) thÃ nh má»™t khá»‘i thá»‘ng nháº¥t, dá»… dÃ ng triá»ƒn khai chá»‰ vá»›i má»™t lá»‡nh Docker.

---

## ğŸ— Architecture

Há»‡ thá»‘ng hoáº¡t Ä‘á»™ng theo mÃ´ hÃ¬nh Microservices, tá»‘i Æ°u hÃ³a cho pháº§n cá»©ng High-End:

```mermaid
graph LR
    User[User / IDE Agent] -->|HTTPS| Nginx
    Nginx -->|Chat UI| O[OpenWebUI]
    Nginx -->|API| L[LiteLLM Gateway]
    
    subgraph "SOLV Core"
    O -->|Middleware| P[Pipelines]
    P -->|Vector Search| Q[Qdrant DB]
    P -->|Web Search| S[SearXNG]
    P -->|Gen Text| L
    L -->|Routing & LB| V[vLLM Cluster]
    end
    
    V -->|Inference| GPU[NVIDIA GPUs]

```

## ğŸ§© Components

| Component | Role | Description |
| --- | --- | --- |
| **S**earXNG | Web Search Tool | CÃ´ng cá»¥ tÃ¬m kiáº¿m áº©n danh, privacy-focused. Cung cáº¥p dá»¯ liá»‡u real-time cho RAG & Agents. |
| **O**penWebUI | Frontend / UI | Giao diá»‡n Chat giá»‘ng ChatGPT, quáº£n lÃ½ User, History vÃ  RAG Pipelines. |
| **L**iteLLM | API Gateway | Router trung tÃ¢m. Chuáº©n hÃ³a má»i request vá» OpenAI Format. CÃ¢n báº±ng táº£i vÃ  log request. |
| **V**LLM | Inference Engine | Engine cháº¡y model nhanh nháº¥t hiá»‡n nay. Há»— trá»£ PagedAttention, Continuous Batching. |
| **Qdrant** | Vector DB | LÆ°u trá»¯ Embedding cho hÃ ng ngÃ n tÃ i liá»‡u doanh nghiá»‡p. |
| **Pipelines** | Logic Middleware | Cho phÃ©p inject Python code Ä‘á»ƒ xá»­ lÃ½ RAG, Function Calling trÆ°á»›c khi gá»i LLM. |

---

## ğŸš€ Quick Start

### 1. Prerequisites

* **OS:** Linux (Ubuntu 22.04/24.04 recommended)
* **GPU Driver:** NVIDIA Driver 535+ & CUDA 12.x
* **Docker:** Docker Engine + **NVIDIA Container Toolkit**

### 2. Installation

Clone repository vÃ  chuáº©n bá»‹ mÃ´i trÆ°á»ng:

```bash
git clone https://github.com/chnghia/solv-stack.git
cd solv-stack

# Táº¡o file mÃ´i trÆ°á»ng tá»« máº«u
cp .env.example .env

```

Táº£i model vá» thÆ° má»¥c local (VÃ­ dá»¥ Qwen3):

```bash
hf download Qwen/Qwen3-Coder-30B-A3B-Instruct --local-dir ./models/Qwen3-Coder-30B


```

Khá»Ÿi cháº¡y há»‡ thá»‘ng:

```bash
docker compose up -d

```

Truy cáº­p:

* **Chat UI:** `http://localhost:8080`
* **API Gateway:** `http://localhost:8080/api`
* **Vector DB:** `http://localhost:8080/qdrant`
* **Web Search:** `http://localhost:8080/search`

---

## âš™ï¸ Configuration

### 1. Model Management (vLLM)

Äá»ƒ thay Ä‘á»•i hoáº·c thÃªm model, chá»‰nh sá»­a `docker-compose.yml` trong service `vllm-backend`:

```yaml
command: >
  --model /models/Llama-3-70B
  --tensor-parallel-size 2  # Sá»‘ lÆ°á»£ng GPU muá»‘n dÃ¹ng
  --gpu-memory-utilization 0.95

```

*LÆ°u Ã½: Cáº§n `docker compose restart vllm-backend` sau khi Ä‘á»•i.*

### 2. Routing Logic (LiteLLM)

Cáº¥u hÃ¬nh táº¡i `litellm_config.yaml`. ÄÃ¢y lÃ  nÆ¡i báº¡n Ä‘á»‹nh nghÄ©a tÃªn model mÃ  Agent/User sáº½ gá»i:

```yaml
model_list:
  - model_name: gpt-4-turbo # Alias giáº£ láº­p
    litellm_params:
      model: openai/llama-3-70b # TÃªn model trong vLLM
      api_base: http://vllm-backend:8000/v1

```

### 3. RAG Pipelines

Code logic xá»­ lÃ½ RAG náº±m trong thÆ° má»¥c `./pipelines`.

* Äá»ƒ kÃ­ch hoáº¡t search web/local doc, vÃ o **OpenWebUI > Admin Panel > Settings > Pipelines** vÃ  báº­t valve tÆ°Æ¡ng á»©ng.

---

## ğŸ“‚ Directory Structure

```text
solv-stack/
â”œâ”€â”€ docker-compose.yml      # Master orchestration file
â”œâ”€â”€ .env                    # Secrets (copy from .env.example)
â”œâ”€â”€ .env.example            # Environment template
â”œâ”€â”€ litellm_config.yaml     # Gateway routing config
â”œâ”€â”€ nginx/                  # Nginx reverse proxy config
â”œâ”€â”€ models/                 # Local LLM Weights (Mounted to vLLM)
â”œâ”€â”€ pipelines/              # Python RAG logic (Mounted to Pipelines container)
â”œâ”€â”€ scripts/                # Helper scripts
â”‚   â”œâ”€â”€ download-model.sh   # Download models from HuggingFace
â”‚   â””â”€â”€ health-check.sh     # Check service status
â”œâ”€â”€ data/                   # Persistent storage (gitignored)
â”‚   â”œâ”€â”€ openwebui/          # User history & settings
â”‚   â”œâ”€â”€ qdrant/             # Vector DB storage
â”‚   â””â”€â”€ searxng/            # Search engine config
â””â”€â”€ README.md

```

## ğŸ›  Scalability & Optimization

* **Multi-GPU:** Há»‡ thá»‘ng máº·c Ä‘á»‹nh cáº¥u hÃ¬nh `tensor-parallel-size` Ä‘á»ƒ chia táº£i model lá»›n lÃªn nhiá»u GPU.
* **Blackwell Optimization:** Náº¿u sá»­ dá»¥ng RTX 6000 Ada/Blackwell, hÃ£y thÃªm flag `--kv-cache-dtype fp8` vÃ o command vLLM Ä‘á»ƒ tÄƒng gáº¥p Ä‘Ã´i context window/throughput.
* **Agent Ready:** LiteLLM Ä‘Ã£ Ä‘Æ°á»£c config Ä‘á»ƒ handle Tool Calling chuáº©n OpenAI, tÆ°Æ¡ng thÃ­ch hoÃ n háº£o vá»›i **CrewAI**, **LangGraph**, vÃ  **VSCode Continue**.

## ğŸ“ License

Internal Use / MIT License.

---

*Built with â¤ï¸ by the AI Engineering Team.*